{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acb617c-7762-42d9-ac45-de301b2ac3b6",
   "metadata": {},
   "source": [
    "### 1. Loading preprocessed Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d903fbc-dc35-465a-8f09-6c2d08764809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('train_data_resamples.csv')\n",
    "testing = pd.read_csv('test_data_resamples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop case ID that contains LUNG-\n",
    "training = training[~training['Case ID'].str.contains(\"LUNG-\")]\n",
    "testing = testing[~testing['Case ID'].str.contains(\"LUNG-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = training['Case ID'].tolist()\n",
    "test_id = testing['Case ID'].tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "training_scaled = scaler.fit_transform(training.drop(['Case ID','Histology'], axis = 1))\n",
    "testing_scaled = scaler.transform(testing.drop(['Case ID','Histology'], axis = 1))\n",
    "\n",
    "# replace the scaled columns\n",
    "training_scaled_pd = pd.DataFrame(training_scaled)\n",
    "training_scaled_pd.columns = training.drop(['Case ID','Histology'], axis = 1).columns\n",
    "\n",
    "testing_scaled_pd = pd.DataFrame(testing_scaled)\n",
    "testing_scaled_pd.columns = testing.drop(['Case ID','Histology'], axis = 1).columns\n",
    "\n",
    "#replace column in training with the scaled columns\n",
    "training_scaled_pd['Case ID'] = train_id\n",
    "training_scaled_pd['Histology'] = training['Histology']\n",
    "\n",
    "testing_scaled_pd['Case ID'] = test_id\n",
    "testing_scaled_pd['Histology'] = testing['Histology']\n",
    "\n",
    "training = training_scaled_pd\n",
    "testing = testing_scaled_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03ea8625-76a1-471f-b4a7-1652674d38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = { 'R01-005','R01-012','R01-013','R01-014','R01-017','R01-021','R01-026','R01-027','R01-028','R01-029',\n",
    "        'R01-038','R01-043','R01-046','R01-048','R01-049','R01-051','R01-052','R01-054','R01-055','R01-056',\n",
    "        'R01-057','R01-059','R01-060','R01-061','R01-062','R01-063','R01-064','R01-065','R01-066','R01-067',\n",
    "        'R01-068','R01-069','R01-071','R01-072','R01-073','R01-076','R01-078','R01-080','R01-081','R01-083',\n",
    "        'R01-084','R01-089','R01-091','R01-093','R01-094','R01-096','R01-097','R01-098','R01-100','R01-101',\n",
    "        'R01-102','R01-103','R01-104','R01-105','R01-106','R01-107','R01-108','R01-109','R01-110','R01-111',\n",
    "        'R01-112','R01-113','R01-114','R01-115','R01-116','R01-117','R01-118','R01-119','R01-120','R01-121',\n",
    "        'R01-122','R01-123','R01-124','R01-125','R01-126','R01-127','R01-128','R01-129','R01-130','R01-131',\n",
    "        'R01-132','R01-133','R01-134','R01-135','R01-136','R01-138','R01-139','R01-140','R01-141','R01-142',\n",
    "        'R01-144','R01-145','R01-146','R01-147','R01-148','R01-149','R01-151','R01-152','R01-154','R01-156',\n",
    "        'R01-157','R01-158','R01-159','R01-160','LUNG-002','LUNG-004','LUNG-006','LUNG-009','LUNG-011',\n",
    "        'LUNG-012','LUNG-018','LUNG-022','LUNG-030','LUNG-042','LUNG-045','LUNG-046','LUNG-047','LUNG-053',\n",
    "        'LUNG-054','LUNG-061','LUNG-063','LUNG-068','LUNG-073','LUNG-078','LUNG-082','LUNG-086','LUNG-093',\n",
    "        'LUNG-098','LUNG-099','LUNG-101','LUNG-104','LUNG-105','LUNG-116','LUNG-122','LUNG-135','LUNG-150',\n",
    "        'LUNG-151','LUNG-173','LUNG-177','LUNG-193','LUNG-201','LUNG-202','LUNG-206','LUNG-208','LUNG-210'}\n",
    "\n",
    "# Base directory\n",
    "#CT Only\n",
    "base_dir = './Lung Mask/'\n",
    "\n",
    "# Store patient ID and corresponding image paths\n",
    "image_paths_per_pid = {}\n",
    "\n",
    "# Loop through each patient ID\n",
    "for pid in PID:\n",
    "    # Use glob to find all images starting with the patient ID\n",
    "    image_paths = glob.glob(os.path.join(base_dir, f'{pid}*'))\n",
    "    \n",
    "    # Concatenate image paths with \";\" and store in the dictionary\n",
    "    image_paths_per_pid[pid] = \";\".join(image_paths)\n",
    "\n",
    "training['CT Images'] = training['Case ID'].map(image_paths_per_pid)\n",
    "testing['CT Images'] = testing['Case ID'].map(image_paths_per_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['CT Images'] = training['CT Images'].str.split(';')\n",
    "testing['CT Images'] = testing['CT Images'].str.split(';')\n",
    "exploded_df_train = training.explode('CT Images')\n",
    "exploded_df_test = testing.explode('CT Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PT_paths = []\n",
    "for i in range(len(exploded_df_train)):\n",
    "    p = exploded_df_train['Case ID'].iloc[i]\n",
    "    frame = exploded_df_train['CT Images'].iloc[i].split('_')[2]\n",
    "    #remove .jpg\n",
    "    frame = frame[:-4]\n",
    "    path = f'./Dataset Lung img/{p}/{p}_{frame}_PT_denoised.jpg'\n",
    "    PT_paths.append(path)\n",
    "\n",
    "exploded_df_train['PT Images'] = PT_paths\n",
    "\n",
    "PT_paths = []\n",
    "for i in range(len(exploded_df_test)):\n",
    "    p = exploded_df_test['Case ID'].iloc[i]\n",
    "    frame = exploded_df_test['CT Images'].iloc[i].split('_')[2]\n",
    "    #remove .jpg\n",
    "    frame = frame[:-4]\n",
    "    path = f'./Dataset Lung img/{p}/{p}_{frame}_PT_denoised.jpg'\n",
    "    PT_paths.append(path)\n",
    "\n",
    "exploded_df_test['PT Images'] = PT_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = exploded_df_train\n",
    "test_data = exploded_df_test\n",
    "train_data['Histology'] = exploded_df_train['Histology']\n",
    "train_data['CT Images'] = exploded_df_train['CT Images']\n",
    "train_data['PT Images'] = exploded_df_train['PT Images']\n",
    "test_data['Histology'] = exploded_df_test['Histology']\n",
    "test_data['CT Images'] = exploded_df_test['CT Images']\n",
    "test_data['PT Images'] = exploded_df_test['PT Images']\n",
    "\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.2, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age at Histological Diagnosis</th>\n",
       "      <th>Weight (lbs)</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Smoking status</th>\n",
       "      <th>Pack Years</th>\n",
       "      <th>%GG</th>\n",
       "      <th>Tumor Location (choice=RUL)</th>\n",
       "      <th>Tumor Location (choice=RML)</th>\n",
       "      <th>Tumor Location (choice=RLL)</th>\n",
       "      <th>...</th>\n",
       "      <th>LMO2</th>\n",
       "      <th>EGR2</th>\n",
       "      <th>BGN</th>\n",
       "      <th>COL4A1</th>\n",
       "      <th>COL5A1</th>\n",
       "      <th>COL5A2</th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Histology</th>\n",
       "      <th>CT Images</th>\n",
       "      <th>PT Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.850335</td>\n",
       "      <td>-0.899936</td>\n",
       "      <td>-1.600781</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-1.615652</td>\n",
       "      <td>0.773389</td>\n",
       "      <td>-1.471960</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820678</td>\n",
       "      <td>-0.499391</td>\n",
       "      <td>-0.168007</td>\n",
       "      <td>-1.001331</td>\n",
       "      <td>-0.872148</td>\n",
       "      <td>0.122013</td>\n",
       "      <td>R01-014</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-014_CT_73.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-014/R01-014_73_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.644348</td>\n",
       "      <td>1.341995</td>\n",
       "      <td>0.624695</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.208628</td>\n",
       "      <td>-0.604210</td>\n",
       "      <td>0.679366</td>\n",
       "      <td>-2.309401</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165363</td>\n",
       "      <td>0.196228</td>\n",
       "      <td>-0.839608</td>\n",
       "      <td>1.920355</td>\n",
       "      <td>-0.832067</td>\n",
       "      <td>-0.774695</td>\n",
       "      <td>R01-094</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-094_CT_72.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-094/R01-094_72_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.561640</td>\n",
       "      <td>-1.513517</td>\n",
       "      <td>-1.600781</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.578891</td>\n",
       "      <td>1.691789</td>\n",
       "      <td>-1.471960</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160929</td>\n",
       "      <td>-0.939248</td>\n",
       "      <td>-0.134072</td>\n",
       "      <td>0.880058</td>\n",
       "      <td>0.106198</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>R01-100</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-100_CT_82.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-100/R01-100_82_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.096171</td>\n",
       "      <td>-0.169185</td>\n",
       "      <td>0.624695</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>-1.60963</td>\n",
       "      <td>0.136117</td>\n",
       "      <td>-0.604210</td>\n",
       "      <td>0.679366</td>\n",
       "      <td>-2.309401</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.418782</td>\n",
       "      <td>-0.333557</td>\n",
       "      <td>-0.793185</td>\n",
       "      <td>1.041334</td>\n",
       "      <td>-0.781890</td>\n",
       "      <td>-0.251079</td>\n",
       "      <td>R01-083</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-083_CT_93.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-083/R01-083_93_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.561640</td>\n",
       "      <td>-1.513517</td>\n",
       "      <td>-1.600781</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.578891</td>\n",
       "      <td>1.691789</td>\n",
       "      <td>-1.471960</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160929</td>\n",
       "      <td>-0.939248</td>\n",
       "      <td>-0.134072</td>\n",
       "      <td>0.880058</td>\n",
       "      <td>0.106198</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>R01-100</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-100_CT_80.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-100/R01-100_80_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.219452</td>\n",
       "      <td>-0.380752</td>\n",
       "      <td>0.624695</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.221387</td>\n",
       "      <td>-0.604210</td>\n",
       "      <td>-1.471960</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.592385</td>\n",
       "      <td>-0.001935</td>\n",
       "      <td>0.091252</td>\n",
       "      <td>0.424699</td>\n",
       "      <td>0.341024</td>\n",
       "      <td>-0.138231</td>\n",
       "      <td>R01-103</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-103_CT_88.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-103/R01-103_88_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.219452</td>\n",
       "      <td>-0.380752</td>\n",
       "      <td>0.624695</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.221387</td>\n",
       "      <td>-0.604210</td>\n",
       "      <td>-1.471960</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.592385</td>\n",
       "      <td>-0.001935</td>\n",
       "      <td>0.091252</td>\n",
       "      <td>0.424699</td>\n",
       "      <td>0.341024</td>\n",
       "      <td>-0.138231</td>\n",
       "      <td>R01-103</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-103_CT_85.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-103/R01-103_85_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.561640</td>\n",
       "      <td>-1.513517</td>\n",
       "      <td>-1.600781</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.578891</td>\n",
       "      <td>1.691789</td>\n",
       "      <td>-1.471960</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160929</td>\n",
       "      <td>-0.939248</td>\n",
       "      <td>-0.134072</td>\n",
       "      <td>0.880058</td>\n",
       "      <td>0.106198</td>\n",
       "      <td>0.022703</td>\n",
       "      <td>R01-100</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-100_CT_72.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-100/R01-100_72_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.644348</td>\n",
       "      <td>0.351179</td>\n",
       "      <td>0.624695</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>-0.221387</td>\n",
       "      <td>-0.604210</td>\n",
       "      <td>0.679366</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585977</td>\n",
       "      <td>-0.375758</td>\n",
       "      <td>-0.249053</td>\n",
       "      <td>0.189485</td>\n",
       "      <td>-0.249335</td>\n",
       "      <td>0.099465</td>\n",
       "      <td>R01-068</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-068_CT_91.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-068/R01-068_91_PT_denoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.123099</td>\n",
       "      <td>0.819507</td>\n",
       "      <td>0.624695</td>\n",
       "      <td>0.318874</td>\n",
       "      <td>1.60963</td>\n",
       "      <td>-0.133449</td>\n",
       "      <td>-0.604210</td>\n",
       "      <td>-1.471960</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.310087</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.506650</td>\n",
       "      <td>-0.947092</td>\n",
       "      <td>0.177380</td>\n",
       "      <td>0.417365</td>\n",
       "      <td>-0.342777</td>\n",
       "      <td>2.969979</td>\n",
       "      <td>R01-051</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-051_CT_94.jpg</td>\n",
       "      <td>./Dataset Lung img/R01-051/R01-051_94_PT_denoi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1004 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age at Histological Diagnosis  Weight (lbs)    Gender  Ethnicity  \\\n",
       "18                      -1.850335     -0.899936 -1.600781   0.318874   \n",
       "34                      -0.644348      1.341995  0.624695   0.318874   \n",
       "17                       0.561640     -1.513517 -1.600781   0.318874   \n",
       "42                      -0.096171     -0.169185  0.624695   0.318874   \n",
       "17                       0.561640     -1.513517 -1.600781   0.318874   \n",
       "..                            ...           ...       ...        ...   \n",
       "14                       1.219452     -0.380752  0.624695   0.318874   \n",
       "14                       1.219452     -0.380752  0.624695   0.318874   \n",
       "17                       0.561640     -1.513517 -1.600781   0.318874   \n",
       "5                       -0.644348      0.351179  0.624695   0.318874   \n",
       "41                       0.123099      0.819507  0.624695   0.318874   \n",
       "\n",
       "    Smoking status  Pack Years       %GG  Tumor Location (choice=RUL)  \\\n",
       "18         0.00000   -1.615652  0.773389                    -1.471960   \n",
       "34         0.00000    1.208628 -0.604210                     0.679366   \n",
       "17         0.00000   -0.578891  1.691789                    -1.471960   \n",
       "42        -1.60963    0.136117 -0.604210                     0.679366   \n",
       "17         0.00000   -0.578891  1.691789                    -1.471960   \n",
       "..             ...         ...       ...                          ...   \n",
       "14         0.00000   -0.221387 -0.604210                    -1.471960   \n",
       "14         0.00000   -0.221387 -0.604210                    -1.471960   \n",
       "17         0.00000   -0.578891  1.691789                    -1.471960   \n",
       "5          0.00000   -0.221387 -0.604210                     0.679366   \n",
       "41         1.60963   -0.133449 -0.604210                    -1.471960   \n",
       "\n",
       "    Tumor Location (choice=RML)  Tumor Location (choice=RLL)  ...      LMO2  \\\n",
       "18                     0.433013                     0.310087  ... -0.820678   \n",
       "34                    -2.309401                     0.310087  ...  0.165363   \n",
       "17                     0.433013                     0.310087  ...  0.160929   \n",
       "42                    -2.309401                     0.310087  ... -0.418782   \n",
       "17                     0.433013                     0.310087  ...  0.160929   \n",
       "..                          ...                          ...  ...       ...   \n",
       "14                     0.433013                     0.310087  ... -0.592385   \n",
       "14                     0.433013                     0.310087  ... -0.592385   \n",
       "17                     0.433013                     0.310087  ...  0.160929   \n",
       "5                      0.433013                     0.310087  ...  0.585977   \n",
       "41                     0.433013                     0.310087  ... -1.506650   \n",
       "\n",
       "        EGR2       BGN    COL4A1    COL5A1    COL5A2  Case ID  Histology  \\\n",
       "18 -0.499391 -0.168007 -1.001331 -0.872148  0.122013  R01-014          0   \n",
       "34  0.196228 -0.839608  1.920355 -0.832067 -0.774695  R01-094          0   \n",
       "17 -0.939248 -0.134072  0.880058  0.106198  0.022703  R01-100          0   \n",
       "42 -0.333557 -0.793185  1.041334 -0.781890 -0.251079  R01-083          0   \n",
       "17 -0.939248 -0.134072  0.880058  0.106198  0.022703  R01-100          0   \n",
       "..       ...       ...       ...       ...       ...      ...        ...   \n",
       "14 -0.001935  0.091252  0.424699  0.341024 -0.138231  R01-103          0   \n",
       "14 -0.001935  0.091252  0.424699  0.341024 -0.138231  R01-103          0   \n",
       "17 -0.939248 -0.134072  0.880058  0.106198  0.022703  R01-100          0   \n",
       "5  -0.375758 -0.249053  0.189485 -0.249335  0.099465  R01-068          0   \n",
       "41 -0.947092  0.177380  0.417365 -0.342777  2.969979  R01-051          0   \n",
       "\n",
       "                        CT Images  \\\n",
       "18  ./Lung Mask/R01-014_CT_73.jpg   \n",
       "34  ./Lung Mask/R01-094_CT_72.jpg   \n",
       "17  ./Lung Mask/R01-100_CT_82.jpg   \n",
       "42  ./Lung Mask/R01-083_CT_93.jpg   \n",
       "17  ./Lung Mask/R01-100_CT_80.jpg   \n",
       "..                            ...   \n",
       "14  ./Lung Mask/R01-103_CT_88.jpg   \n",
       "14  ./Lung Mask/R01-103_CT_85.jpg   \n",
       "17  ./Lung Mask/R01-100_CT_72.jpg   \n",
       "5   ./Lung Mask/R01-068_CT_91.jpg   \n",
       "41  ./Lung Mask/R01-051_CT_94.jpg   \n",
       "\n",
       "                                            PT Images  \n",
       "18  ./Dataset Lung img/R01-014/R01-014_73_PT_denoi...  \n",
       "34  ./Dataset Lung img/R01-094/R01-094_72_PT_denoi...  \n",
       "17  ./Dataset Lung img/R01-100/R01-100_82_PT_denoi...  \n",
       "42  ./Dataset Lung img/R01-083/R01-083_93_PT_denoi...  \n",
       "17  ./Dataset Lung img/R01-100/R01-100_80_PT_denoi...  \n",
       "..                                                ...  \n",
       "14  ./Dataset Lung img/R01-103/R01-103_88_PT_denoi...  \n",
       "14  ./Dataset Lung img/R01-103/R01-103_85_PT_denoi...  \n",
       "17  ./Dataset Lung img/R01-100/R01-100_72_PT_denoi...  \n",
       "5   ./Dataset Lung img/R01-068/R01-068_91_PT_denoi...  \n",
       "41  ./Dataset Lung img/R01-051/R01-051_94_PT_denoi...  \n",
       "\n",
       "[1004 rows x 56 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf91788-a438-4e15-9a28-2980b7424e50",
   "metadata": {},
   "source": [
    "### 2. MedClip Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7afc9d9-27f3-4f89-bf4e-b60d83d62337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the MedCLIP model and processor\n",
    "# Note: Replace the model and processor names with actual MedCLIP model names when available.\n",
    "model = CLIPModel.from_pretrained(\"flaviagiammarino/pubmed-clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"flaviagiammarino/pubmed-clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "train_data[\"Histopathological Grade\"] = label_encoder.fit_transform(train_data[\"Histopathological Grade\"])\n",
    "\n",
    "# Example: Extracting a row of features and label\n",
    "example_row = train_data.iloc[0]  # Extract the first row of the dataframe\n",
    "#features = torch.tensor([example_row[['Age at Histological Diagnosis', 'Smoking status', 'Pathological T stage', 'Pathological N stage', 'Pathological M stage', 'EGFR mutation status', 'Histopathological Grade', 'Recurrence','KRAS mutation status']].values])  # Replace with your actual feature column names\n",
    "label = example_row['Histology']\n",
    "\n",
    "# Extract features and ensure they are in a suitable numeric format\n",
    "features_np = example_row.drop(['Case ID', 'Histology', 'CT Images', 'PT Images']).values\n",
    "\n",
    "\n",
    "# Ensure data is in a suitable format (e.g., float)\n",
    "features_np = features_np.astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "features = torch.tensor([features_np])\n",
    "\n",
    "# Example text and image\n",
    "text = [\"A patient with a history of lung cancer\"]\n",
    "image_path = \"./Fused Lung 2 copy/R01-149_74_Fused.jpg\"  # Replace with the path to your CT image\n",
    "image = Image.open(image_path)\n",
    "image_path_2 = \"./Dataset Lung img/R01-149/R01-149_74_PT_denoised.jpg\"  # Replace with the path to your CT image\n",
    "image_2 = Image.open(image_path_2)\n",
    "# Preprocess the text and image\n",
    "inputs = processor(\n",
    "    text=text, \n",
    "    images=image, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    ")\n",
    "\n",
    "inputs_2 = processor(\n",
    "    text=text, \n",
    "    images=image_2, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    ")\n",
    "\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "\n",
    "outputs_2 = model(**inputs_2)\n",
    "# Extract image and text features\n",
    "image_features = outputs.image_embeds\n",
    "image_features_2 = outputs_2.image_embeds\n",
    "# Concatenate image features and tabular data\n",
    "combined_features = torch.cat((image_features, image_features_2, features), dim=1)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(256, 128)        # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, 64)         # Third hidden layer\n",
    "        self.fc4 = nn.Linear(64, 32)          # Fourth hidden layer\n",
    "        self.fc5 = nn.Linear(32, 1)           # Output layer\n",
    "        \n",
    "        # Dropout for regularization (can adjust rate as needed)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Activation for first hidden layer\n",
    "        x = self.dropout(x)      # Dropout for first hidden layer\n",
    "        x = F.relu(self.fc2(x))  # Activation for second hidden layer\n",
    "        x = self.dropout(x)      # Dropout for second hidden layer\n",
    "        x = F.relu(self.fc3(x))  # Activation for third hidden layer\n",
    "        x = self.dropout(x)      # Dropout for third hidden layer\n",
    "        x = F.relu(self.fc4(x))  # Activation for fourth hidden layer\n",
    "        x = self.dropout(x)      # Dropout for fourth hidden layer\n",
    "        x = torch.sigmoid(self.fc5(x))  # Sigmoid activation for output layer\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the classifier\n",
    "input_dim = combined_features.size(1)\n",
    "classifier = Classifier(input_dim)\n",
    "\n",
    "# Forward pass through the classifier\n",
    "predictions = classifier(combined_features)\n",
    "\n",
    "# Applying a threshold to get class labels\n",
    "threshold = 0.5\n",
    "predicted_labels = (predictions >= threshold).int()\n",
    "\n",
    "# Convert labels to numpy array or list for further usage\n",
    "predicted_labels_np = predicted_labels.detach().cpu().numpy()\n",
    "\n",
    "# Compute accuracy by comparing predictions to actual labels\n",
    "accuracy = accuracy_score([label], predicted_labels_np)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2936165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),  # Random rotation between -10 and 10 degrees\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flipping with probability 0.5\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Vertical flipping with probability 0.5\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),  # Random cropping and resizing\n",
    "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "    #transforms.Normalize(mean=[mean], std=[std]),\n",
    "])\n",
    "\n",
    "class NSLCDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        cancer_type = 'Adenocarcinoma' if row['Histology'] == 0 else 'Squamous cell carcinoma'\n",
    "        text = [\"A photo of \" + cancer_type + \" cancer\"]\n",
    "        image_paths = row['CT Images'].split(';')\n",
    "        pt = row['PT Images'].split(';')\n",
    "        image_paths.extend(pt)\n",
    "        images = [Image.open(img_path) for img_path in image_paths]\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "        \n",
    "        # Process each image and text separately and stack image tensors\n",
    "        inputs_list = [self.processor(text=text, images=image, return_tensors=\"pt\", padding='max_length', max_length=32) for image in images]\n",
    "\n",
    "        image_tensors = torch.stack([inp['pixel_values'].squeeze(0) for inp in inputs_list], dim=0)\n",
    "        # Average or sum the image features if you're using multiple images\n",
    "        inputs = {\n",
    "            'input_ids': inputs_list[0]['input_ids'],  # Using the text input from the first item\n",
    "            'pixel_values': image_tensors.mean(dim=0).unsqueeze(0)  # Averaging the image inputs and adding batch dimension\n",
    "        }\n",
    "        \n",
    "\n",
    "        features_np = row.drop(['Case ID', 'Histology', 'CT Images', 'PT Images']).values.astype(np.float32)\n",
    "        \n",
    "        features = torch.tensor([features_np])\n",
    "        label = torch.tensor(row['Histology']).float()\n",
    "        return inputs, features, label\n",
    "\n",
    "#train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "#valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Creating Datasets\n",
    "train_dataset = NSLCDataset(train_data, processor)\n",
    "\n",
    "valid_dataset = NSLCDataset(valid_data, processor)\n",
    "test_dataset = NSLCDataset(test_data, processor)\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "classifier = Classifier(564)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for inputs, features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].squeeze(1)\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.image_embeds\n",
    "        features = features.squeeze(1)\n",
    "        combined_features = torch.cat((image_features, features), dim=1)\n",
    "        predictions = classifier(combined_features)\n",
    "        if (predictions.squeeze().shape != labels.shape):\n",
    "            continue\n",
    "        # print(predictions.squeeze().shape)\n",
    "        # print(labels.shape)\n",
    "        loss = criterion(predictions.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(torch.round(predictions).cpu().detach().numpy())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Validation Loop\n",
    "# classifier.eval()\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "# with torch.no_grad():\n",
    "#     for inputs, features, labels in valid_loader:\n",
    "#         inputs['pixel_values'] = inputs['pixel_values'].squeeze(1)\n",
    "#         inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n",
    "#         outputs = model(**inputs)\n",
    "#         image_features = outputs.image_embeds\n",
    "#         features = features.squeeze(1)\n",
    "#         combined_features = torch.cat((image_features, features), dim=1)\n",
    "#         predictions = classifier(combined_features)\n",
    "#         all_preds.append(predictions.squeeze().cpu().numpy())\n",
    "#         all_labels.append(labels.cpu().numpy())\n",
    "# all_preds = np.concatenate(all_preds)\n",
    "# all_labels = np.concatenate(all_labels)\n",
    "# accuracy = accuracy_score(all_labels, all_preds >= 0.5)\n",
    "# print(f\"Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, features, labels in test_loader:\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].squeeze(1)\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.image_embeds\n",
    "        features = features.squeeze(1)\n",
    "        combined_features = torch.cat((image_features, features), dim=1)\n",
    "        predictions = classifier(combined_features)\n",
    "        all_preds.append(predictions.squeeze().cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "accuracy = accuracy_score(all_labels, all_preds >= 0.5)\n",
    "print(f\"Testing Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 75.89%\n"
     ]
    }
   ],
   "source": [
    "classifier.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, features, labels in test_loader:\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].squeeze(1)\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.image_embeds\n",
    "        features = features.squeeze(1)\n",
    "        combined_features = torch.cat((image_features, features), dim=1)\n",
    "        predictions = classifier(combined_features)\n",
    "        predictions_numpy = predictions.squeeze().cpu().numpy()\n",
    "        labels_numpy = labels.cpu().numpy()\n",
    "        all_preds.append(predictions.squeeze().cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        count += 1\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "accuracy = accuracy_score(all_labels, all_preds >= 0.5)\n",
    "print(f\"Testing Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.80      0.76      0.78       612\n",
      "     Class 1       0.72      0.75      0.73       487\n",
      "\n",
      "    accuracy                           0.76      1099\n",
      "   macro avg       0.76      0.76      0.76      1099\n",
      "weighted avg       0.76      0.76      0.76      1099\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "binary_predictions = (all_preds >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "print(classification_report(all_labels, binary_predictions, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(all_labels, binary_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './models/MedClip_CT_2D.pth')\n",
    "torch.save(model.state_dict(), './models/MedClip_CT_2D_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
