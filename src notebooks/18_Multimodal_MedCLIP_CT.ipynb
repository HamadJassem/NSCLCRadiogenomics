{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acb617c-7762-42d9-ac45-de301b2ac3b6",
   "metadata": {},
   "source": [
    "### 1. Loading preprocessed Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d903fbc-dc35-465a-8f09-6c2d08764809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv('train_data_resamples.csv')\n",
    "testing = pd.read_csv('test_data_resamples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_id = training['Case ID'].tolist()\n",
    "test_id = testing['Case ID'].tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "training_scaled = scaler.fit_transform(training.drop(['Case ID','Histology'], axis = 1))\n",
    "testing_scaled = scaler.transform(testing.drop(['Case ID','Histology'], axis = 1))\n",
    "\n",
    "# replace the scaled columns\n",
    "training_scaled_pd = pd.DataFrame(training_scaled)\n",
    "training_scaled_pd.columns = training.drop(['Case ID','Histology'], axis = 1).columns\n",
    "\n",
    "testing_scaled_pd = pd.DataFrame(testing_scaled)\n",
    "testing_scaled_pd.columns = testing.drop(['Case ID','Histology'], axis = 1).columns\n",
    "\n",
    "#replace column in training with the scaled columns\n",
    "training_scaled_pd['Case ID'] = train_id\n",
    "training_scaled_pd['Histology'] = training['Histology']\n",
    "\n",
    "testing_scaled_pd['Case ID'] = test_id\n",
    "testing_scaled_pd['Histology'] = testing['Histology']\n",
    "\n",
    "training = training_scaled_pd\n",
    "testing = testing_scaled_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03ea8625-76a1-471f-b4a7-1652674d38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PID = { 'R01-005','R01-012','R01-013','R01-014','R01-017','R01-021','R01-026','R01-027','R01-028','R01-029',\n",
    "        'R01-038','R01-043','R01-046','R01-048','R01-049','R01-051','R01-052','R01-054','R01-055','R01-056',\n",
    "        'R01-057','R01-059','R01-060','R01-061','R01-062','R01-063','R01-064','R01-065','R01-066','R01-067',\n",
    "        'R01-068','R01-069','R01-071','R01-072','R01-073','R01-076','R01-078','R01-080','R01-081','R01-083',\n",
    "        'R01-084','R01-089','R01-091','R01-093','R01-094','R01-096','R01-097','R01-098','R01-100','R01-101',\n",
    "        'R01-102','R01-103','R01-104','R01-105','R01-106','R01-107','R01-108','R01-109','R01-110','R01-111',\n",
    "        'R01-112','R01-113','R01-114','R01-115','R01-116','R01-117','R01-118','R01-119','R01-120','R01-121',\n",
    "        'R01-122','R01-123','R01-124','R01-125','R01-126','R01-127','R01-128','R01-129','R01-130','R01-131',\n",
    "        'R01-132','R01-133','R01-134','R01-135','R01-136','R01-138','R01-139','R01-140','R01-141','R01-142',\n",
    "        'R01-144','R01-145','R01-146','R01-147','R01-148','R01-149','R01-151','R01-152','R01-154','R01-156',\n",
    "        'R01-157','R01-158','R01-159','R01-160','LUNG-002','LUNG-004','LUNG-006','LUNG-009','LUNG-011',\n",
    "        'LUNG-012','LUNG-018','LUNG-022','LUNG-030','LUNG-042','LUNG-045','LUNG-046','LUNG-047','LUNG-053',\n",
    "        'LUNG-054','LUNG-061','LUNG-063','LUNG-068','LUNG-073','LUNG-078','LUNG-082','LUNG-086','LUNG-093',\n",
    "        'LUNG-098','LUNG-099','LUNG-101','LUNG-104','LUNG-105','LUNG-116','LUNG-122','LUNG-135','LUNG-150',\n",
    "        'LUNG-151','LUNG-173','LUNG-177','LUNG-193','LUNG-201','LUNG-202','LUNG-206','LUNG-208','LUNG-210'}\n",
    "\n",
    "# Base directory\n",
    "#CT Only\n",
    "base_dir = './Lung Mask/'\n",
    "\n",
    "# Store patient ID and corresponding image paths\n",
    "image_paths_per_pid = {}\n",
    "\n",
    "# Loop through each patient ID\n",
    "for pid in PID:\n",
    "    # Use glob to find all images starting with the patient ID\n",
    "    image_paths = glob.glob(os.path.join(base_dir, f'{pid}*'))\n",
    "    \n",
    "    # Concatenate image paths with \";\" and store in the dictionary\n",
    "    image_paths_per_pid[pid] = \";\".join(image_paths)\n",
    "\n",
    "training['Images'] = training['Case ID'].map(image_paths_per_pid)\n",
    "testing['Images'] = testing['Case ID'].map(image_paths_per_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training['Images'] = training['Images'].str.split(';')\n",
    "testing['Images'] = testing['Images'].str.split(';')\n",
    "exploded_df_train = training.explode('Images')\n",
    "exploded_df_test = testing.explode('Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = exploded_df_train\n",
    "test_data = exploded_df_test\n",
    "train_data['Histology'] = exploded_df_train['Histology']\n",
    "train_data['Images'] = exploded_df_train['Images']\n",
    "test_data['Histology'] = exploded_df_test['Histology']\n",
    "test_data['Images'] = exploded_df_test['Images']\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.2, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age at Histological Diagnosis</th>\n",
       "      <th>Weight (lbs)</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Smoking status</th>\n",
       "      <th>Pack Years</th>\n",
       "      <th>%GG</th>\n",
       "      <th>Tumor Location (choice=RUL)</th>\n",
       "      <th>Tumor Location (choice=RML)</th>\n",
       "      <th>Tumor Location (choice=RLL)</th>\n",
       "      <th>...</th>\n",
       "      <th>VIM</th>\n",
       "      <th>LMO2</th>\n",
       "      <th>EGR2</th>\n",
       "      <th>BGN</th>\n",
       "      <th>COL4A1</th>\n",
       "      <th>COL5A1</th>\n",
       "      <th>COL5A2</th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Histology</th>\n",
       "      <th>Images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.142478</td>\n",
       "      <td>-0.520045</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>-0.865494</td>\n",
       "      <td>-0.216051</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>-2.263846</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.245929</td>\n",
       "      <td>0.051644</td>\n",
       "      <td>-1.315155</td>\n",
       "      <td>-0.914313</td>\n",
       "      <td>-0.894344</td>\n",
       "      <td>-0.720289</td>\n",
       "      <td>-0.829919</td>\n",
       "      <td>LUNG-105</td>\n",
       "      <td>1</td>\n",
       "      <td>./Lung Mask/LUNG-105_93.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.399769</td>\n",
       "      <td>-0.390782</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>0.529459</td>\n",
       "      <td>-0.442795</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250353</td>\n",
       "      <td>-0.505755</td>\n",
       "      <td>-0.494601</td>\n",
       "      <td>-0.519134</td>\n",
       "      <td>-0.459968</td>\n",
       "      <td>0.332892</td>\n",
       "      <td>-0.040866</td>\n",
       "      <td>R01-115</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-115_CT_78.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.091412</td>\n",
       "      <td>0.520355</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>-0.151018</td>\n",
       "      <td>-0.442795</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.645176</td>\n",
       "      <td>0.347385</td>\n",
       "      <td>-1.164960</td>\n",
       "      <td>-0.440282</td>\n",
       "      <td>-0.639265</td>\n",
       "      <td>-0.270667</td>\n",
       "      <td>-0.085214</td>\n",
       "      <td>LUNG-054</td>\n",
       "      <td>1</td>\n",
       "      <td>./Lung Mask/LUNG-054_80.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.142478</td>\n",
       "      <td>2.262414</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>-1.392433</td>\n",
       "      <td>1.298577</td>\n",
       "      <td>-0.442795</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>-2.263846</td>\n",
       "      <td>-2.263846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295248</td>\n",
       "      <td>-0.480444</td>\n",
       "      <td>0.615903</td>\n",
       "      <td>-0.350723</td>\n",
       "      <td>0.695070</td>\n",
       "      <td>0.677702</td>\n",
       "      <td>0.172543</td>\n",
       "      <td>LUNG-086</td>\n",
       "      <td>1</td>\n",
       "      <td>./Lung Mask/LUNG-086_77.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.091412</td>\n",
       "      <td>-0.148482</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>-0.636630</td>\n",
       "      <td>0.119566</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>...</td>\n",
       "      <td>2.944803</td>\n",
       "      <td>1.827347</td>\n",
       "      <td>0.612770</td>\n",
       "      <td>-0.309901</td>\n",
       "      <td>2.111564</td>\n",
       "      <td>0.775107</td>\n",
       "      <td>-0.792705</td>\n",
       "      <td>R01-113</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-113_CT_91.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.362535</td>\n",
       "      <td>-0.633082</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>-0.247934</td>\n",
       "      <td>-0.442795</td>\n",
       "      <td>-1.179086</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>...</td>\n",
       "      <td>2.893910</td>\n",
       "      <td>-0.520695</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>0.051088</td>\n",
       "      <td>0.575107</td>\n",
       "      <td>0.107287</td>\n",
       "      <td>-0.131911</td>\n",
       "      <td>R01-103</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-103_CT_85.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>-0.535331</td>\n",
       "      <td>0.638638</td>\n",
       "      <td>-1.581139</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>1.050609</td>\n",
       "      <td>-0.442795</td>\n",
       "      <td>-1.179086</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192031</td>\n",
       "      <td>-0.572238</td>\n",
       "      <td>-0.084814</td>\n",
       "      <td>0.106664</td>\n",
       "      <td>-0.083595</td>\n",
       "      <td>0.316285</td>\n",
       "      <td>-0.147813</td>\n",
       "      <td>LUNG-006</td>\n",
       "      <td>1</td>\n",
       "      <td>./Lung Mask/LUNG-006_74.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.549164</td>\n",
       "      <td>-1.925348</td>\n",
       "      <td>-1.581139</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>-0.636630</td>\n",
       "      <td>2.369009</td>\n",
       "      <td>-1.179086</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561548</td>\n",
       "      <td>-0.113635</td>\n",
       "      <td>-0.982738</td>\n",
       "      <td>-0.124799</td>\n",
       "      <td>1.083835</td>\n",
       "      <td>-0.057580</td>\n",
       "      <td>0.046347</td>\n",
       "      <td>R01-100</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-100_CT_72.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.942017</td>\n",
       "      <td>0.201910</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>0.271694</td>\n",
       "      <td>-0.247934</td>\n",
       "      <td>-0.442795</td>\n",
       "      <td>0.848115</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>...</td>\n",
       "      <td>1.994819</td>\n",
       "      <td>0.116045</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>-0.214552</td>\n",
       "      <td>0.312325</td>\n",
       "      <td>-0.307193</td>\n",
       "      <td>0.131372</td>\n",
       "      <td>R01-068</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-068_CT_91.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.006916</td>\n",
       "      <td>0.736182</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.238176</td>\n",
       "      <td>1.935821</td>\n",
       "      <td>-0.152323</td>\n",
       "      <td>-0.442795</td>\n",
       "      <td>-1.179086</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>0.441726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.992035</td>\n",
       "      <td>-1.014728</td>\n",
       "      <td>-0.991071</td>\n",
       "      <td>0.118319</td>\n",
       "      <td>0.566913</td>\n",
       "      <td>-0.372797</td>\n",
       "      <td>3.310881</td>\n",
       "      <td>R01-051</td>\n",
       "      <td>0</td>\n",
       "      <td>./Lung Mask/R01-051_CT_94.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3073 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age at Histological Diagnosis  Weight (lbs)    Gender  Ethnicity  \\\n",
       "84                       0.142478     -0.520045  0.632456   0.238176   \n",
       "43                      -0.399769     -0.390782  0.632456   0.238176   \n",
       "71                       1.091412      0.520355  0.632456   0.238176   \n",
       "78                       0.142478      2.262414  0.632456   0.238176   \n",
       "36                       1.091412     -0.148482  0.632456   0.238176   \n",
       "..                            ...           ...       ...        ...   \n",
       "14                       1.362535     -0.633082  0.632456   0.238176   \n",
       "59                      -0.535331      0.638638 -1.581139   0.238176   \n",
       "17                       0.549164     -1.925348 -1.581139   0.238176   \n",
       "5                       -0.942017      0.201910  0.632456   0.238176   \n",
       "41                       0.006916      0.736182  0.632456   0.238176   \n",
       "\n",
       "    Smoking status  Pack Years       %GG  Tumor Location (choice=RUL)  \\\n",
       "84        0.271694   -0.865494 -0.216051                     0.848115   \n",
       "43        0.271694    0.529459 -0.442795                     0.848115   \n",
       "71        0.271694   -0.151018 -0.442795                     0.848115   \n",
       "78       -1.392433    1.298577 -0.442795                     0.848115   \n",
       "36        0.271694   -0.636630  0.119566                     0.848115   \n",
       "..             ...         ...       ...                          ...   \n",
       "14        0.271694   -0.247934 -0.442795                    -1.179086   \n",
       "59        0.271694    1.050609 -0.442795                    -1.179086   \n",
       "17        0.271694   -0.636630  2.369009                    -1.179086   \n",
       "5         0.271694   -0.247934 -0.442795                     0.848115   \n",
       "41        1.935821   -0.152323 -0.442795                    -1.179086   \n",
       "\n",
       "    Tumor Location (choice=RML)  Tumor Location (choice=RLL)  ...       VIM  \\\n",
       "84                     0.441726                    -2.263846  ... -1.245929   \n",
       "43                     0.441726                     0.441726  ... -0.250353   \n",
       "71                     0.441726                     0.441726  ... -0.645176   \n",
       "78                    -2.263846                    -2.263846  ... -0.295248   \n",
       "36                     0.441726                     0.441726  ...  2.944803   \n",
       "..                          ...                          ...  ...       ...   \n",
       "14                     0.441726                     0.441726  ...  2.893910   \n",
       "59                     0.441726                     0.441726  ... -0.192031   \n",
       "17                     0.441726                     0.441726  ...  0.561548   \n",
       "5                      0.441726                     0.441726  ...  1.994819   \n",
       "41                     0.441726                     0.441726  ... -0.992035   \n",
       "\n",
       "        LMO2      EGR2       BGN    COL4A1    COL5A1    COL5A2   Case ID  \\\n",
       "84  0.051644 -1.315155 -0.914313 -0.894344 -0.720289 -0.829919  LUNG-105   \n",
       "43 -0.505755 -0.494601 -0.519134 -0.459968  0.332892 -0.040866   R01-115   \n",
       "71  0.347385 -1.164960 -0.440282 -0.639265 -0.270667 -0.085214  LUNG-054   \n",
       "78 -0.480444  0.615903 -0.350723  0.695070  0.677702  0.172543  LUNG-086   \n",
       "36  1.827347  0.612770 -0.309901  2.111564  0.775107 -0.792705   R01-113   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "14 -0.520695  0.012946  0.051088  0.575107  0.107287 -0.131911   R01-103   \n",
       "59 -0.572238 -0.084814  0.106664 -0.083595  0.316285 -0.147813  LUNG-006   \n",
       "17 -0.113635 -0.982738 -0.124799  1.083835 -0.057580  0.046347   R01-100   \n",
       "5   0.116045 -0.384157 -0.214552  0.312325 -0.307193  0.131372   R01-068   \n",
       "41 -1.014728 -0.991071  0.118319  0.566913 -0.372797  3.310881   R01-051   \n",
       "\n",
       "    Histology                         Images  \n",
       "84          1    ./Lung Mask/LUNG-105_93.jpg  \n",
       "43          0  ./Lung Mask/R01-115_CT_78.jpg  \n",
       "71          1    ./Lung Mask/LUNG-054_80.jpg  \n",
       "78          1    ./Lung Mask/LUNG-086_77.jpg  \n",
       "36          0  ./Lung Mask/R01-113_CT_91.jpg  \n",
       "..        ...                            ...  \n",
       "14          0  ./Lung Mask/R01-103_CT_85.jpg  \n",
       "59          1    ./Lung Mask/LUNG-006_74.jpg  \n",
       "17          0  ./Lung Mask/R01-100_CT_72.jpg  \n",
       "5           0  ./Lung Mask/R01-068_CT_91.jpg  \n",
       "41          0  ./Lung Mask/R01-051_CT_94.jpg  \n",
       "\n",
       "[3073 rows x 55 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf91788-a438-4e15-9a28-2980b7424e50",
   "metadata": {},
   "source": [
    "### 2. MedClip Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7afc9d9-27f3-4f89-bf4e-b60d83d62337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the MedCLIP model and processor\n",
    "# Note: Replace the model and processor names with actual MedCLIP model names when available.\n",
    "model = CLIPModel.from_pretrained(\"flaviagiammarino/pubmed-clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"flaviagiammarino/pubmed-clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "train_data[\"Histopathological Grade\"] = label_encoder.fit_transform(train_data[\"Histopathological Grade\"])\n",
    "\n",
    "# Example: Extracting a row of features and label\n",
    "example_row = train_data.iloc[0]  # Extract the first row of the dataframe\n",
    "#features = torch.tensor([example_row[['Age at Histological Diagnosis', 'Smoking status', 'Pathological T stage', 'Pathological N stage', 'Pathological M stage', 'EGFR mutation status', 'Histopathological Grade', 'Recurrence','KRAS mutation status']].values])  # Replace with your actual feature column names\n",
    "label = example_row['Histology']\n",
    "\n",
    "# Extract features and ensure they are in a suitable numeric format\n",
    "features_np = example_row.drop(['Case ID', 'Histology', 'Images']).values\n",
    "\n",
    "\n",
    "# Ensure data is in a suitable format (e.g., float)\n",
    "features_np = features_np.astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "features = torch.tensor([features_np])\n",
    "\n",
    "# Example text and image\n",
    "text = [\"A patient with a history of lung cancer\"]\n",
    "image_path = \"./Fused Lung 2 copy/R01-149_74_Fused.jpg\"  # Replace with the path to your CT image\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the text and image\n",
    "inputs = processor(\n",
    "    text=text, \n",
    "    images=image, \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Extract image and text features\n",
    "image_features = outputs.image_embeds\n",
    "\n",
    "# Concatenate image features and tabular data\n",
    "combined_features = torch.cat((image_features, features), dim=1)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(256, 128)        # Second hidden layer\n",
    "        self.fc3 = nn.Linear(128, 64)         # Third hidden layer\n",
    "        self.fc4 = nn.Linear(64, 32)          # Fourth hidden layer\n",
    "        self.fc5 = nn.Linear(32, 1)           # Output layer\n",
    "        \n",
    "        # Dropout for regularization (can adjust rate as needed)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Activation for first hidden layer\n",
    "        x = self.dropout(x)      # Dropout for first hidden layer\n",
    "        x = F.relu(self.fc2(x))  # Activation for second hidden layer\n",
    "        x = self.dropout(x)      # Dropout for second hidden layer\n",
    "        x = F.relu(self.fc3(x))  # Activation for third hidden layer\n",
    "        x = self.dropout(x)      # Dropout for third hidden layer\n",
    "        x = F.relu(self.fc4(x))  # Activation for fourth hidden layer\n",
    "        x = self.dropout(x)      # Dropout for fourth hidden layer\n",
    "        x = torch.sigmoid(self.fc5(x))  # Sigmoid activation for output layer\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the classifier\n",
    "input_dim = combined_features.size(1)\n",
    "classifier = Classifier(input_dim)\n",
    "\n",
    "# Forward pass through the classifier\n",
    "predictions = classifier(combined_features)\n",
    "\n",
    "# Applying a threshold to get class labels\n",
    "threshold = 0.5\n",
    "predicted_labels = (predictions >= threshold).int()\n",
    "\n",
    "# Convert labels to numpy array or list for further usage\n",
    "predicted_labels_np = predicted_labels.detach().cpu().numpy()\n",
    "\n",
    "# Compute accuracy by comparing predictions to actual labels\n",
    "accuracy = accuracy_score([label], predicted_labels_np)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2936165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=10),  # Random rotation between -10 and 10 degrees\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flipping with probability 0.5\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # Vertical flipping with probability 0.5\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),  # Random cropping and resizing\n",
    "    transforms.ToTensor(),  # Convert PIL image to PyTorch tensor\n",
    "    #transforms.Normalize(mean=[mean], std=[std]),\n",
    "])\n",
    "\n",
    "class NSLCDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        cancer_type = 'Adenocarcinoma' if row['Histology'] == 0 else 'Squamous cell carcinoma'\n",
    "        text = [\"A photo of \" + cancer_type + \" cancer\"]\n",
    "        # Assuming 'Images' column contains lists of image paths\n",
    "        image_paths = row['Images'].split(';')\n",
    "        images = [Image.open(img_path) for img_path in image_paths]\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "        \n",
    "        # Process each image and text separately and stack image tensors\n",
    "        inputs_list = [self.processor(text=text, images=image, return_tensors=\"pt\", padding='max_length', max_length=32) for image in images]\n",
    "\n",
    "        image_tensors = torch.stack([inp['pixel_values'].squeeze(0) for inp in inputs_list], dim=0)\n",
    "        # Average or sum the image features if you're using multiple images\n",
    "        inputs = {\n",
    "            'input_ids': inputs_list[0]['input_ids'],  # Using the text input from the first item\n",
    "            'pixel_values': image_tensors.mean(dim=0).unsqueeze(0)  # Averaging the image inputs and adding batch dimension\n",
    "        }\n",
    "        \n",
    "\n",
    "        features_np = row.drop(['Case ID', 'Histology', 'Images']).values.astype(np.float32)\n",
    "        \n",
    "        features = torch.tensor([features_np])\n",
    "        label = torch.tensor(row['Histology']).float()\n",
    "        return inputs, features, label\n",
    "\n",
    "\n",
    "# Creating Datasets\n",
    "train_dataset = NSLCDataset(train_data, processor)\n",
    "\n",
    "valid_dataset = NSLCDataset(valid_data, processor)\n",
    "test_dataset = NSLCDataset(test_data, processor)\n",
    "\n",
    "# Creating DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 5\n",
    "\n",
    "# Use in loss function\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    for inputs, features, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].squeeze(1)\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.image_embeds\n",
    "        features = features.squeeze(1)\n",
    "        combined_features = torch.cat((image_features, features), dim=1)\n",
    "        predictions = classifier(combined_features)\n",
    "        if (predictions.squeeze().shape != labels.shape):\n",
    "            continue\n",
    "        # print(predictions.squeeze().shape)\n",
    "        # print(labels.shape)\n",
    "        loss = criterion(predictions.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(torch.round(predictions).cpu().detach().numpy())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Validation Loop\n",
    "classifier.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, features, labels in valid_loader:\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].squeeze(1)\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.image_embeds\n",
    "        features = features.squeeze(1)\n",
    "        combined_features = torch.cat((image_features, features), dim=1)\n",
    "        predictions = classifier(combined_features)\n",
    "        all_preds.append(predictions.squeeze().cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "accuracy = accuracy_score(all_labels, all_preds >= 0.5)\n",
    "print(f\"Validation Accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "\n",
    "classifier.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, features, labels in test_loader:\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].squeeze(1)\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.image_embeds\n",
    "        features = features.squeeze(1)\n",
    "        combined_features = torch.cat((image_features, features), dim=1)\n",
    "        predictions = classifier(combined_features)\n",
    "        all_preds.append(predictions.squeeze().cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "accuracy = accuracy_score(all_labels, all_preds >= 0.5)\n",
    "print(f\"Testing Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "count = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, features, labels in test_loader:\n",
    "        inputs['pixel_values'] = inputs['pixel_values'].squeeze(1)\n",
    "        inputs['input_ids'] = inputs['input_ids'].squeeze(1)\n",
    "        outputs = model(**inputs)\n",
    "        image_features = outputs.image_embeds\n",
    "        features = features.squeeze(1)\n",
    "        combined_features = torch.cat((image_features, features), dim=1)\n",
    "        predictions = classifier(combined_features)\n",
    "        predictions_numpy = predictions.squeeze().cpu().numpy()\n",
    "        labels_numpy = labels.cpu().numpy()\n",
    "        all_preds.append(predictions.squeeze().cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        count += 1\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "accuracy = accuracy_score(all_labels, all_preds >= 0.5)\n",
    "print(f\"Testing Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "binary_predictions = (all_preds >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "print(classification_report(all_labels, binary_predictions, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "#Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(all_labels, binary_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './models/MedClip_CT_2D.pth')\n",
    "torch.save(model.state_dict(), './models/MedClip_CT_2D_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
